{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Data Science Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I used data from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) to train a model to classify whether a given image is an airplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model shape\n",
    "The model takes in an input with shape (1, 32, 32, 3), corresponding to a 32x32 size RGB image. The training and test data needed to be pre-processed to fit this shape.\n",
    "\n",
    "The model contains a data augmentation layer, which randomly transforms the inputs to create more data to train from.\n",
    "\n",
    "The model contains a normalisation layer to normalise the values between 0 and 1.\n",
    "\n",
    "The model contains three 2D convolution layers and three max pooling layers. The number of nodes in theses were determined by hyperparameter tuning.\n",
    "\n",
    "The model contains a dropout layer to drop nodes and avoid overfitting.\n",
    "\n",
    "The model contains a flattening layer to flatten the data into a 1D array.\n",
    "\n",
    "The model contains two dense layers to downsample the data into two output values.\n",
    "\n",
    "The output of the model has shape (2, ), where the first value contains the logit value that the image is not an airplane while the second value contains the logit value that the image is an airplane. The output needed to be post-processed into probabilities.\n",
    "\n",
    "## Loss and optimizer\n",
    "I picked the crossentropy loss function. It is useful for classification models with probabilities as outputs. I set the from_logits argument to True as the output of my model are logit values instead of probabilities.\n",
    "\n",
    "I picked the Adam optimizer method. It is a stochastic gradient descent method. This method is well suited for large problems. During hyperparameter tuning, I also noticed that the Adam optimizer gave more accurate models than the SGD optimizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "![accuracy](images/epoch_accuracy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "![loss](images/epoch_loss.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "The training metrics are shown in orange and the validation metrics are shown in blue.\n",
    "\n",
    "Initially, the accuracy increases and the loss drops relatively quickly. At this stage, the model is underfitted, so there is a lot of room for improvement in the metrics. However, after a while, the improvement slows down, particularly for the validation metrics. Eventually, validation accuracy and loss stagnate and stop improving, while training accuracy and loss keep improving. This is because the model is being overfitted to the training data and it doesn't generalise well to other data.\n",
    "\n",
    "Overall the model reached a validation accuracy of about 90% and a validation loss of about 24% before the validation metrics stopped improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I trained a model on the CIFAR-10 dataset to predict whether a given image is an airplane or not. The performance of the resulting model was relatively decent, with 90% validation accuracy. However, the modelling process could be improved by building more efficient pipelines so that training the model didn't take so long. Deploying and training the model on a cloud service such as Microsoft Azure may allow more intensive training of the model. It would also be helpful and insightful if I understood what each layer actually means instead of blindly using layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Learning Multiple Layers of Features from Tiny Images](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Data Science Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I used data from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) to train a model to classify whether a given image is an airplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary\n",
    "\n",
    "| Layer (type)                    | Output Shape       | Param #\n",
    "----------------------------------|--------------------|-------\n",
    "| sequential_3 (Sequential)       | (None, 32, 32, 3)  | 0         \n",
    "| Normaliser (Rescaling)          | (None, 32, 32, 3)  | 0         \n",
    "| conv2d_9 (Conv2D)               | (None, 32, 32, 32) | 896       \n",
    "| max_pooling2d_9 (MaxPooling2D)  | (None, 16, 16, 32) | 0         \n",
    "| conv2d_10 (Conv2D)              | (None, 16, 16, 48) | 13872     \n",
    "| max_pooling2d_10 (MaxPooling2D) | (None, 8, 8, 48)   | 0         \n",
    "| conv2d_11 (Conv2D)              | (None, 8, 8, 84)   | 36372     \n",
    "| max_pooling2d_11 (MaxPooling2D) | (None, 4, 4, 84)   | 0         \n",
    "| dropout_3 (Dropout)             | (None, 4, 4, 84)   | 0         \n",
    "| flatten_3 (Flatten)             | (None, 1344)       | 0         \n",
    "| dense_6 (Dense)                 | (None, 100)        | 134500    \n",
    "| dense_7 (Dense)                 | (None, 2)          | 202       \n",
    "                                                                 \n",
    "Total params: 185,842\n",
    "\n",
    "Trainable params: 185,842\n",
    "\n",
    "Non-trainable params: 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Shape\n",
    "The model takes in an input with shape (1, 32, 32, 3), corresponding to a 32x32 size RGB image. The training and test data needed to be pre-processed to fit this shape.\n",
    "\n",
    "The model contains a data augmentation layer, which randomly transforms the inputs to create more data to train from.\n",
    "\n",
    "The model contains a normalisation layer to normalise the values between 0 and 1.\n",
    "\n",
    "The model contains three 2D convolution layers and three max pooling layers. The number of nodes in theses were determined by hyperparameter tuning.\n",
    "\n",
    "The model contains a dropout layer to drop nodes and avoid overfitting.\n",
    "\n",
    "The model contains a flattening layer to flatten the data into a 1D array.\n",
    "\n",
    "The model contains two dense layers to downsample the data into two output values.\n",
    "\n",
    "The output of the model has shape (2, ), where the first value contains the logit value that the image is not an airplane while the second value contains the logit value that the image is an airplane. The output needed to be post-processed into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizer\n",
    "I picked the crossentropy loss function. It is useful for classification models with probabilities as outputs. I set the from_logits argument to True as the output of my model are logit values instead of probabilities.\n",
    "\n",
    "I picked the Adam optimizer method. It is a stochastic gradient descent method. This method is well suited for large problems. During hyperparameter tuning, I also noticed that the Adam optimizer gave more accurate models than the SGD optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "![accuracy](images/epoch_accuracy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "![loss](images/epoch_loss.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "The training metrics are shown in orange and the validation metrics are shown in blue.\n",
    "\n",
    "Generally, the accuracy increases and the loss decreases over time as the model gets trained on more data. Accuracy and loss are linked to each other; accuracy represents the number of correct predictions while loss represents the error between the target and predicted value, which takes into account the confidence of predictions. As the training and test data sets are balanced between the two classes, accuracy is a good metric.\n",
    "\n",
    "Initially, the accuracy increases and the loss drops relatively quickly. At this stage, the model hasn't trained on a lot of data and is underfitted, so there is a lot of room for improvement in the metrics. However, after training on more data for a while, the improvement slows down, particularly for the validation metrics. Eventually, validation accuracy and loss stagnate and stop improving, while training accuracy and loss keep improving. This is because the model is being overfitted to the training data and it doesn't generalise well to other data.\n",
    "\n",
    "Overall, my final model reached a validation accuracy of 0.91 and a validation loss of 0.24 before the validation metrics stopped improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I trained a model on the CIFAR-10 dataset to predict whether a given image is an airplane or not. The performance of my final model was relatively decent, with 90% validation accuracy.\n",
    "\n",
    "However, the modelling process could be improved by building more efficient pipelines so that training the model didn't take so long. Deploying and training the model on a cloud service such as Microsoft Azure may allow more intensive training of the model.\n",
    "\n",
    "It would also be helpful and insightful if I understood what each layer actually means, so that I could make more informed decisions on the shape of the model and conduct hyperparameter searches more strategically, such that I would be able to further improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. [Learning Multiple Layers of Features from Tiny Images](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf), Alex Krizhevsky, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
